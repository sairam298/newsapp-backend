<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Get started with Red Hat OpenShift Connectors</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/09/get-started-red-hat-openshift-connectors" /><author><name>Bernard Tison</name></author><id>063945e9-6a30-43ef-8a28-d6f112c9bfec</id><updated>2022-06-09T10:45:00Z</updated><published>2022-06-09T10:45:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors&lt;/a&gt; is a new cloud service offering from Red Hat. They are pre-built connectors for quick and reliable connectivity across data, services, and systems. Connectors are delivered as a fully managed service, tightly integrated with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;, Red Hat's managed cloud service for Apache Kafka.&lt;/p&gt; &lt;p&gt;Red Hat OpenShift Connectors is in the Service Preview phase at the moment. As part of the Service Preview program, you can deploy up to four connectors free of charge. The connectors are deleted after 48 hours.&lt;/p&gt; &lt;p&gt;We make a distinction between &lt;em&gt;source&lt;/em&gt; and &lt;em&gt;sink&lt;/em&gt; connectors. A source connector allows you to send data from an external system to OpenShift Streams for Apache Kafka. A sink connector allows you to send data from OpenShift Streams for Apache Kafka to an external system.&lt;/p&gt; &lt;p&gt;At the moment we offer more than 50 source and sink connectors. These include source and sink connectors to a variety of cloud services based on the awesome &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K&lt;/a&gt; technology, as well as source connectors for databases based on the popular &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; project that performs change data capture. The complete list of available connectors to date can be found at the &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors site&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article shows you how to get started with OpenShift Connectors. You will learn how to create a source connector and a sink connector and send data to and from topics in OpenShift Streams for Apache Kafka.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes that you have created an OpenShift Streams for Apache Kafka instance and that the instance is in the &lt;code&gt;Ready&lt;/code&gt; state. Please refer to &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with OpenShift Streams for Apache Kafka&lt;/a&gt; for step-by-step instructions to create your Kafka instance.&lt;/p&gt; &lt;h2&gt;Configure OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;Your OpenShift Streams for Apache Kafka instance requires some setup for use with Red Hat OpenShift Connectors. This includes creating &lt;em&gt;Kafka topics&lt;/em&gt; to store messages sent by producers (data sources) and make them available to consumers (data sinks), creating a &lt;em&gt;service account&lt;/em&gt; to allow you to connect and authenticate your connectors with the Kafka instance, and setting up &lt;em&gt;access rules&lt;/em&gt; for the service account to define how your connectors can access and use the associated Kafka instance topics.&lt;/p&gt; &lt;h3&gt;Create a Kafka Topic&lt;/h3&gt; &lt;p&gt;We start with an example Kafka topic, because it's the resource at the center of Kafka operations. Create a topic as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log in to the hybrid cloud console at &lt;a href="https://console.redhat.com"&gt;console.redhat.com&lt;/a&gt; with your Red Hat account credentials.&lt;/li&gt; &lt;li&gt;Navigate to &lt;strong&gt;Application and Data Services→Streams for Apache Kafka→Kafka instances&lt;/strong&gt;, and select the Kafka instance you created as part of the prerequisites.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Topics&lt;/strong&gt; tab, and click &lt;strong&gt;Create topic&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter a unique name for your topic, for example &lt;strong&gt;test-topic&lt;/strong&gt;. Accept the defaults for partitions, message retention, and replicas.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Create a service account&lt;/h3&gt; &lt;p&gt;Next, you need an account in order to get access to Kafka. Create the account as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Service Accounts&lt;/strong&gt; and click &lt;strong&gt;Create Service Account&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter a unique name for the service account, and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Copy the generated &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; to a secure location. You'll use these credentials when configuring your connectors.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;I have copied the client ID and secret&lt;/strong&gt; option, and then click &lt;strong&gt;Close&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Set the access level for the service account&lt;/h3&gt; &lt;p&gt;Having an account, you can now obtain the necessary permissions. For this example, you need to be a consumer for one service and a producer for another, so you'll enable both sets of permissions. Set the access levels as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Streams for Apache Kafka→Kafka instances&lt;/strong&gt;. Select the Kafka instance you created as part of the prerequisites.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Access&lt;/strong&gt; tab to view the current Access Control List (ACL) for the Kafka instance and then click &lt;strong&gt;Manage access&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;From the &lt;strong&gt;Account&lt;/strong&gt; drop-down menu, select the service account that you created in the previous step, and then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;Assign Permissions&lt;/strong&gt;, click &lt;strong&gt;Add permission&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;From the drop-down menu, select &lt;strong&gt;Consume from a topic&lt;/strong&gt;. Set all resource identifiers to &lt;code&gt;is&lt;/code&gt; and all identifier values to &lt;code&gt;*&lt;/code&gt; (an asterisk).&lt;/li&gt; &lt;li&gt;From the drop-down menu, select &lt;strong&gt;Produce to a topic.&lt;/strong&gt; Set all resource identifiers to &lt;code&gt;is&lt;/code&gt; and all identifier values to &lt;code&gt;*&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Figure 1 shows what the access control list should look like.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafka-acl.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kafka-acl.png?itok=znBuZtpN" width="1440" height="778" alt="The access control list for your Streams for Apache Kafka instance shows that your service account has access to all topics." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The access control list for your Streams for Apache Kafka instance shows that your service account has access to all topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The access control list for your OpenShift Streams for Apache Kafka instance shows that your service account has access to all topics.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Your OpenShift Streams for Apache instance is now configured for use by your connectors.&lt;/p&gt; &lt;h2&gt;Create a source connector&lt;/h2&gt; &lt;p&gt;A source connector consumes events from an external data source and produces Kafka messages. For this getting started guide, you will use the Data Generator source connector. This connector does not actually consume data from an external system, but produces Kafka messages to a topic at a configurable interval. Install the connector as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Connectors&lt;/strong&gt; and click &lt;strong&gt;Create a Connectors instance&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the connector that you want to use as a data source. You can browse through the catalog of available connectors, or search for a particular connector by name and filter the search to look for sink or source connectors. For example, to find the Data Generator source connector, type &lt;strong&gt;data&lt;/strong&gt; in the search box. The list filters to show only the Data Generator&lt;strong&gt; &lt;/strong&gt;connector card, as shown in Figure 2. Click the &lt;strong&gt;Data Generator &lt;/strong&gt;card to select the connector, then click &lt;strong&gt;Next&lt;/strong&gt;. &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/data-generatour-source-connector.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/data-generatour-source-connector.png?itok=87IJPYr_" width="600" height="312" alt="Search for a Connector by name" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Search for a Connector by name. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;For the &lt;strong&gt;Kafka instance&lt;/strong&gt;, click the card for the OpenShift Streams for Apache Kafka instance that you configured for your connectors, and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Namespace&lt;/strong&gt; page, click &lt;strong&gt;Create preview namespace&lt;/strong&gt; to provision a namespace for hosting the connector instances that you create. This evaluation namespace will remain available for 48 hours. You can create up to four connector instances per namespace. Once the namespace is available, select it and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Configure the core configuration for your connector as follows: &lt;ul&gt; &lt;li&gt; &lt;p&gt;Provide a name for the connector.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enter the &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; of the service account that you created for your connectors, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide connector-specific configuration values. For the Data Generator connector, provide the following information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data shape Format&lt;/strong&gt;: Accept the default, &lt;code&gt;application/octet-stream&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Topic Names&lt;/strong&gt;: Enter the name of the topic that you created previously for your connectors (for example, &lt;strong&gt;test-topic&lt;/strong&gt;).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Content Type&lt;/strong&gt;: Accept the default, &lt;code&gt;text/plain&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Message&lt;/strong&gt;: Enter the content of the message that you want the connector instance to send to the Kafka topic. For example, type &lt;code&gt;Hello World!&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Period&lt;/strong&gt;: Specify the interval (in milliseconds) at which you want the connector instance to send messages to the Kafka topic. For example, specify &lt;code&gt;10000&lt;/code&gt; to send a message every 10 seconds.&lt;/p&gt; &lt;p&gt;Figure 3 shows an overview of the connector configuration.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/data-generator-source-connector-configuration.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/data-generator-source-connector-configuration.png?itok=kbGu-5kF" width="600" height="419" alt="The Data Generator connector requires specific configuration information." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Data Generator connector requires specific configuration information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Configure the error handling policy for your connector instance. The default is &lt;strong&gt;stop&lt;/strong&gt;, which causes the connector to shut down when it encounters an error.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Review the summary of the configuration properties and click &lt;strong&gt;Create Connector&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your connector instance should now be listed in the table of connectors. After a couple of seconds, the status of your connector instance will change to the &lt;code&gt;Ready&lt;/code&gt; state, and it will start producing messages and sending them to its associated Kafka topic.&lt;/p&gt; &lt;p&gt;From the connectors table, you can stop, start, and delete your connector, as well as edit its configuration, by clicking the options icon (three vertical dots).&lt;/p&gt; &lt;h2&gt;Create a sink connector&lt;/h2&gt; &lt;p&gt;A sink connector consumes messages from a Kafka topic and sends them to an external system. For this example, use the HTTP Sink connector, which consumes Kafka messages from one or more topics and sends the messages to an HTTP endpoint.&lt;/p&gt; &lt;p&gt;The Webhook.site service offers a convenient way to obtain a general-purpose HTTP endpoint. Open a new tab in your browser and navigate to &lt;a href="https://webhook.site"&gt;https://webhook.site&lt;/a&gt;. The page displays a unique URL that you can use as a data sink, as shown in Figure 4.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/webhook-site.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/webhook-site.png?itok=kTROqrN-" width="1440" height="671" alt="Copy the long, unique URL generated for you by Webhook.site." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Copy the long, unique URL generated for you by Webhook.site. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once you have that URL, configure the endpoint as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Connectors&lt;/strong&gt; and click &lt;strong&gt;Create Connectors instance&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;To find the &lt;strong&gt;HTTP Sink&lt;/strong&gt; connector, enter &lt;strong&gt;http&lt;/strong&gt; in the search field. Click the &lt;strong&gt;HTTP Sink&lt;/strong&gt; connector card and then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the OpenShift Streams for Apache Kafka instance for the connector to work with.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Namespace&lt;/strong&gt; page, click the &lt;strong&gt;eval&lt;/strong&gt; namespace that you created when you created the source connector. Then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Configure the core configuration for your connector: &lt;ul&gt; &lt;li&gt;Provide a unique name for the connector.&lt;/li&gt; &lt;li&gt;Type the &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; of the service account that you created for your connectors, and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Provide the connector-specific configuration for your connector. For the &lt;strong&gt;HTTP sink connector&lt;/strong&gt;, provide the following information: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Data shape Format&lt;/strong&gt;: Accept the default, &lt;code&gt;application/octet-stream&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method&lt;/strong&gt;: Accept the default, &lt;code&gt;POST&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;URL&lt;/strong&gt;: Type your unique URL from Webhook.site.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Topic Names&lt;/strong&gt;: Type the name of the topic that you used for the source connector.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Set the error handling policy to &lt;strong&gt;stop&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Review the summary of the configuration properties and click &lt;strong&gt;Create Connector&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your connector instance will be added to the table of connectors. After a couple of seconds, the status of your connector instance will change to the &lt;code&gt;Ready&lt;/code&gt; state. The connector consumes messages from the associated Kafka topic and sends them to the HTTP endpoint.&lt;/p&gt; &lt;p&gt;Visit the unique URL you got from Webhook.site in your browser to see the HTTP POST calls with the &lt;code&gt;"Hello World!"&lt;/code&gt; messages that you defined in the source connector, as shown in Figure 5.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/webhook-site-posts.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/webhook-site-posts.png?itok=tyQ_TpCB" width="1440" height="500" alt="Your Webhook.site URL displays the HTTP POST messages received." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Your Webhook.site URL displays the HTTP POST messages received. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Congratulations—you have created your first Red Hat OpenShift Connector instances. From here, you can create other source and sink connectors to a variety of external systems and cloud services.&lt;/p&gt; &lt;p&gt;Stay tuned for the next article in this series, in which we will show you how to use Red Hat OpenShift Connectors to connect to a cloud-based database and capture data change events from that database.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/09/get-started-red-hat-openshift-connectors" title="Get started with Red Hat OpenShift Connectors"&gt;Get started with Red Hat OpenShift Connectors&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2022-06-09T10:45:00Z</dc:date></entry><entry><title>Detecting nondeterministic test cases with Bunsen</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/09/detecting-nondeterministic-test-cases-bunsen" /><author><name>Serhei Makarov</name></author><id>29f395a8-179b-4628-8ea9-4e837ac160f1</id><updated>2022-06-09T07:00:00Z</updated><published>2022-06-09T07:00:00Z</published><summary type="html">&lt;p&gt;Many open source projects have test suites that include nondeterministic test cases with unpredictable behavior. Tests might be nondeterministic because they launch several parallel processes or threads that interact in an unpredictable manner, or because they depend on some activity in the operating system that has nondeterministic behavior. The presence of these tests can interfere with automated regression checking in &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD pipelines&lt;/a&gt;. This article shows how to automate the discovery of nondeterministic test cases using a short Python script based on the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=summary"&gt; Bunsen&lt;/a&gt; test suite analysis toolkit.&lt;/p&gt; &lt;h2&gt;The problem: Finding nondeterministic ("flaky") test cases&lt;/h2&gt; &lt;p&gt;Test cases in an open source project's test suite can have nondeterministic behavior and produce different outcomes when run repeatedly. Such test cases are commonly referred to as &lt;em&gt;flaky,&lt;/em&gt; and their presence in a test suite tends to complicate the evaluation of test results. Without additional investigation, a PASS or FAIL outcome for a nondeterministic test case doesn't conclusively prove the presence or absence of a problem.&lt;/p&gt; &lt;p&gt;Nondeterministic test cases are usually found in test suites of projects such as SystemTap and the GNU Debugger (GDB) because they provide value when testing the project's functionality under ideal conditions. Rewriting these test suites to eliminate nondeterminism would be a large and low-priority task tying up a large amount of scarce developer time. Therefore, it's worthwhile to develop tools to analyze test results from a project and identify nondeterministic test cases. A developer reading test results could use this analysis to recognize nondeterministic test cases and interpret their outcomes separately from outcomes of reliable test cases.&lt;/p&gt; &lt;p&gt;In a previous article,&lt;a href="https://developers.redhat.com/blog/2021/05/10/automating-the-testing-process-for-systemtap-part-2-test-result-analysis-with-bunsen"&gt; Automating the testing process for SystemTap, Part 2: Test result analysis with Bunsen&lt;/a&gt;, I described Bunsen, a tool that collects a set of test result log files from a project and stores them in a deduplicated Git repository together with an index in JSON format. Bunsen also provides a Python library for accessing the data in this repository. These capabilities can be used to implement a script to detect nondeterministic test cases.&lt;/p&gt; &lt;h2&gt;Developing the script&lt;/h2&gt; &lt;p&gt;The overall strategy of the script is to find test cases that have been run multiple times on the same system configuration with varying outcomes. Such test cases are likely to be nondeterministic.&lt;/p&gt; &lt;h3&gt;Basic setup&lt;/h3&gt; &lt;p&gt;The analysis script starts by importing and initializing the Bunsen library:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;1 #!/usr/bin/env python3 2 info="""Detect nondeterministic testcases that yield different outcomes when tested 3 multiple times on the same configuration.""" 4 from bunsen import Bunsen, BunsenOptions 5 if __name__=='__main__': 6 BunsenOptions.add_option('source_repo', group='source_repo', 7 cmdline='source-repo', default=None, 8 help_str="Use project commit history from Git repo &lt;path&gt;", 9 help_cookie="&lt;path&gt;") 10 BunsenOptions.add_option('branch', group='source_repo', default=None, 11 help_str="Use project commit history from &lt;branch&gt; in source_repo", 12 help_cookie="&lt;branch&gt;") 13 BunsenOptions.add_option('project', group='filtering', default=None, 14 help_str="Restrict the analysis to testruns in &lt;projects&gt;", 15 help_cookie="&lt;projects&gt;") 16 17 import git 18 import tqdm 19 from common.utils import * # add_list, add_set 20 if __name__=='__main__': 21 22 b, opts = Bunsen.from_cmdline(info=info) 23 projects = opts.get_list('project', default=b.projects) 24 repo = git.Repo(opts.source_repo) for # Iterate all testruns in projects # Collect information from the testrun # Print the results&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A Bunsen analysis script is a Python program that imports the &lt;code&gt;bunsen&lt;/code&gt; module. Lines 5-15 of the preceding script define the following options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;source_repo&lt;/code&gt; identifies a Git repository containing up-to-date source code for the project. The commit history of this repository identifies the relative version order of test runs.&lt;/li&gt; &lt;li&gt;&lt;code&gt;branch&lt;/code&gt; identifies a branch within &lt;code&gt;source_repo&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;project&lt;/code&gt; names a project within the Bunsen repository, and is present because the Bunsen repository can store test results from more than one project. Test results from separate projects are stored in separate branches, and the analysis script can be instructed to scan and compare test results from a single project or from a subset of the projects. If this option is omitted, all test runs in the Bunsen repository will be scanned.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Options for an analysis script can be passed as command-line arguments or specified in the Bunsen repository's configuration file. For example, if the Bunsen repository is stored under &lt;code&gt;/path/to/bunsen/.bunsen&lt;/code&gt;, the configuration file is located at &lt;code&gt;/path/to/bunsen/.bunsen/config&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The second part of the script (lines 20-24) instantiates the following objects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;b&lt;/code&gt;, an instance of the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=bunsen/repo.py;h=fa6868d9380523891ddac848a2e4651cd5f0dfa1;hb=3fce15d750f78e0f2c54040638aa4e57b2cc3803#l309"&gt;Bunsen&lt;/a&gt; class providing access to the Bunsen repository&lt;/li&gt; &lt;li&gt;&lt;code&gt;opts&lt;/code&gt;, an instance of the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=bunsen/repo.py;h=fa6868d9380523891ddac848a2e4651cd5f0dfa1;hb=3fce15d750f78e0f2c54040638aa4e57b2cc3803#l1811"&gt;BunsenOptions&lt;/a&gt; class providing access to the script's options&lt;/li&gt; &lt;li&gt;&lt;code&gt;repo&lt;/code&gt;, an instance of the &lt;code&gt;git.Repo&lt;/code&gt; class from the &lt;a href="https://gitpython.readthedocs.io/en/stable/"&gt;GitPython library&lt;/a&gt;, providing access to the version history of the project in the &lt;code&gt;source_repo&lt;/code&gt; repository.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Collecting the test results&lt;/h3&gt; &lt;p&gt;A test case is considered to be nondeterministic if it was tested more than once on the same &lt;code&gt;source_repo&lt;/code&gt; commit and system configuration with varying outcomes—a PASS outcome in one test run and a FAIL outcome in another, for instance. To determine which test cases produce varying outcomes, the script gathers a list of test runs for each commit and configuration. The script then iterates through the test runs for each combination and compares the outcomes of each test case for different test runs. The script uses a dictionary named &lt;code&gt;all_testruns&lt;/code&gt; to store the list of test runs corresponding to each commit and configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;26 all_testruns = {} # maps (commit, config) -&gt; list(Testrun) 27 28 for testrun in b.testruns(opts.projects): 29 commit, config = testrun.get_source_commit(), testrun.get_config() 30 if commit is None: continue 31 add_list(all_testruns, (commit,config), testrun) for # Iterate all (commit, config) # Iterate the set of testruns matching (commit, config), # and compare the outcome of each testcase to detect nondeterminism # Print the results&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;An instance of the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=bunsen/model.py;h=e9ccea97b4cacb2b6b42f6e79de55b7a2a077fa5;hb=3fce15d750f78e0f2c54040638aa4e57b2cc3803#l1070"&gt;Testrun&lt;/a&gt; class in the Bunsen library represents a single test run. The instance provides access to the commit that was tested, the system configuration, and the outcomes of individual test cases. The &lt;code&gt;all_testruns&lt;/code&gt; dictionary, defined on line 26, maps a (commit, config) pair to a list of &lt;code&gt;Testrun&lt;/code&gt; instances.&lt;/p&gt; &lt;p&gt;For each test run, the loop invokes the utility method &lt;code&gt;add_list&lt;/code&gt; at line 31 to add the test run to the dictionary. The &lt;code&gt;add_list&lt;/code&gt; method is a simple utility method that appends a value to a list stored at a specified key:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;def add_list(d,k,v): if k not in d: d[k] = [] d[k].append(v)&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Identifying the nondeterministic test cases&lt;/h3&gt; &lt;p&gt;Next, the script iterates over the list of &lt;code&gt;Testrun&lt;/code&gt; objects for each commit and configuration. To record the list of test cases that produced varying outcomes, the script uses a second dictionary named &lt;code&gt;known_flakes&lt;/code&gt;, whose keys are (testcase, config) pairs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;26 all_testruns = {} # maps (commit, config) -&gt; list(Testrun) 27 28 for testrun in b.testruns(opts.projects): 29 commit, config = testrun.get_source_commit(), testrun.get_config() 30 if commit is None: continue 31 add_list(all_testruns, (commit,config), testrun) 32 33 known_flakes = {} # maps (tc_info, config) -&gt; set(commit) 34 # where tc_info is (name, subtest, outcome) 35 36 for commit, config in tqdm.tqdm(all_testruns, \ 37 desc="Scanning configurations", unit="configs"): if len(all_testruns[commit, config]) &lt;= 1: continue # no possibility of flakes commit_testcases = {} # maps tc_info -&gt; list(Testrun) for testrun in all_testruns[commit, config]: # Gather the list of failing tc_info tuples appearing in testrun for # each tc_info tuple that appears in some testrun": # Check whether the failing tuple appears in all testruns; # If not, mark the tuple as a flake # Print the results&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second loop, iterating over the commits and configurations, could take a long time. So the script uses the Python &lt;a href="https://tqdm.github.io/"&gt;tqdm&lt;/a&gt; library to display a progress bar (lines 36-37).&lt;/p&gt; &lt;p&gt;After the remaining code is filled in, the second loop appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;… 36 for commit, config in tqdm.tqdm(all_testruns, \ 37 desc="Scanning configurations", unit="configs"): 38 39 if len(all_testruns[commit, config]) &lt;= 1: 40 continue # no possibility of flakes 41 42 commit_testcases = {} # maps tc_info -&gt; list(Testrun) 43 44 for testrun in all_testruns[commit, config]: 45 for tc in testrun.testcases: 46 if tc.is_pass(): continue 47 tc_info = (tc.name, tc.outcome, tc.subtest) 48 add_list(commit_testcases, tc_info, testrun) 49 50 expected_testruns = len(all_testruns[commit, config]) 51 for tc_info in commit_testcases: 52 if len(commit_testcases[tc_info]) &lt; n_testruns: 53 # XXX tc_info didn't appear in all runs 54 add_set(known_flakes, tc_info, commit) …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second loop skips (commit, config) pairs for which only one test run was found (lines 39-40). For each of the other test runs, the loop iterates over its test case outcomes and gathers a list of test failures that appear in the test run. Test case outcomes are represented by instances of Bunsen's &lt;code&gt;Testcase&lt;/code&gt; class. In accordance with the &lt;a href="https://ftp.gnu.org/old-gnu/Manuals/dejagnu-1.3/html_mono/dejagnu.html"&gt;DejaGNU framework's test result model&lt;/a&gt;, a &lt;code&gt;Testcase&lt;/code&gt; object has fields called 'name' (the name of the top-level Expect file defining the test case), 'outcome' (one of the &lt;a href="https://ftp.gnu.org/old-gnu/Manuals/dejagnu-1.3/html_mono/dejagnu.html#SEC6"&gt;standard POSIX outcome codes&lt;/a&gt;, such as &lt;code&gt;PASS&lt;/code&gt;, &lt;code&gt;FAIL&lt;/code&gt;, or &lt;code&gt;UNTESTED&lt;/code&gt;), and 'subtest' (a string providing additional information about the outcome).&lt;/p&gt; &lt;p&gt;A third dictionary named &lt;code&gt;commit_testcases&lt;/code&gt; stores failing test case outcomes. The dictionary maps the (name, outcome, subtest) tuple describing the test failure to a list of test runs where this tuple was found to occur. The script assembles &lt;code&gt;commit_testcases&lt;/code&gt; on lines 44-48 and iterates over it on lines 51-54 to collect every (name, outcome, subtest) tuple that appeared in some test runs but not all of them. Such a tuple fits our definition of a varying test outcome and therefore is stored in the &lt;code&gt;known_flakes&lt;/code&gt; dictionary. The &lt;code&gt;known_flakes&lt;/code&gt; dictionary maps each (testcase, config) combination to a set of commit IDs on which that combination was found to produce varying outcomes.&lt;/p&gt; &lt;h3&gt;Reporting the nondeterministic test cases&lt;/h3&gt; &lt;p&gt;Having accumulated a list of suspected nondeterministic tests in the &lt;code&gt;known_flakes&lt;/code&gt; dictionary, the script iterates through it and prints the nondeterministic tests' outcomes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;56 sorted_tc = [] 57 for tc_info in all_testcases: 58 sorted_tc.append((tc_info, all_testcases[tc_info])) 59 sorted_tc.sort(reverse=True, key=lambda tup: len(tup[1])) 60 for tc_info, commits in sorted_tc: 61 print(len(commits),"commits have nondeterministic",tc_info)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script sorts the test outcomes (on lines 56-59) in decreasing order of frequency: test cases found to produce varying outcomes on a larger number of commits are printed first. An additional loop can be added to print the commits on which the test outcomes were found to be nondeterministic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;60 for tc_info, commits in sorted_tc: 61 print(len(commits),"commits have nondeterministic",tc_info) 62 for hexsha in commits: 63 commit = repo.commit(hexsha) 64 print("*",commit.hexsha[:7],commit.summary)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lines 63-64 use the GitPython library and the &lt;code&gt;git.Repo&lt;/code&gt; object that was instantiated at the beginning of the script to retrieve a summary of the commit message.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=scripts-main/find_flakes.py;h=debc60f7b00718c1fe7d01e97afe9689e9d957dd;hb=HEAD"&gt;completed analysis script&lt;/a&gt; is less than 100 lines of Python code. When tested on a modest laptop (2.3GHz i3-6100U), the script took approximately 42 seconds with a maximum resident memory size of 285MB to scan a Bunsen repository from the SystemTap project containing data from 4,158 test runs across 693 commits. Within that Bunsen repository, 368 (commit, config) pairs were tested by more than one test run and provided useful data for the analysis script. In practice, more complex analysis scripts that compare test case results over time (rather than within the same commit) will tend to have larger RAM requirements.&lt;/p&gt; &lt;p&gt;When run, the analysis script produces output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;72 commits have nondeterministic ('systemtap.base/attach_detach.exp', 'FAIL: attach_detach (initial load) - EOF\n', 'FAIL') 72 commits have nondeterministic ('systemtap.base/attach_detach.exp', 'FAIL: attach_detach (initial load) - begin seen (1 0)\n', 'FAIL') 61 commits have nondeterministic ('systemtap.examples/check.exp', 'FAIL: systemtap.examples/io/ioblktime run\n', 'FAIL') 51 commits have nondeterministic ('systemtap.base/utrace_p5.exp', 'FAIL: UTRACE_P5_07 unexpected output (after passing output)\n', 'FAIL') 47 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 32-bit madvise tp_syscall\n', 'FAIL') 40 commits have nondeterministic ('systemtap.base/abort.exp', 'FAIL: abort: TEST 6: abort() in timer.profile (using globals): stdout: string should be "fire 3!\\nfire 2!\\nfire 1!\\n", but got "fire 2!\n', 'FAIL') 39 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 64-bit clock tp_syscall\n', 'FAIL') 39 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 32-bit clock tp_syscall\n', 'FAIL') 38 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 32-bit socket tp_syscall\n', 'FAIL') 37 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_disabled_iter_5 (invalid output)\n', 'FAIL') 37 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_timer_50ms (invalid output)\n', 'FAIL') 36 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 64-bit madvise tp_syscall\n', 'FAIL') 34 commits have nondeterministic ('systemtap.bpf/nonbpf.exp', 'FAIL: bigmap1.stp unexpected output\n', 'FAIL') 33 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_timer_10ms (invalid output)\n', 'FAIL') 33 commits have nondeterministic ('systemtap.bpf/bpf.exp', 'FAIL: timer2.stp incorrect result\n', 'FAIL') 33 commits have nondeterministic ('systemtap.bpf/bpf.exp', 'KFAIL: bigmap1.stp unexpected output (PRMS: BPF)\n', 'KFAIL') 33 commits have nondeterministic ('systemtap.bpf/bpf.exp', 'FAIL: stat3.stp incorrect result\n', 'FAIL') 33 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_timer_100ms (invalid output)\n', 'FAIL') 32 commits have nondeterministic ('systemtap.server/client.exp', 'FAIL: New trusted servers matches after reinstatement by ip address\n', 'FAIL') 32 commits have nondeterministic ('systemtap.unprivileged/unprivileged_myproc.exp', 'FAIL: unprivileged myproc: --unprivileged process.thread.end\n', 'FAIL') 32 commits have nondeterministic ('systemtap.base/procfs_bpf.exp', 'FAIL: PROCFS_BPF initial value: cat: /var/tmp/systemtap-root/PROCFS_BPF/command: No such file or directory\n', 'FAIL') 32 commits have nondeterministic ('systemtap.base/abort.exp', 'FAIL: abort: TEST 6: abort() in timer.profile (using globals): stdout: string should be "fire 3!\\nfire 2!\\nfire 1!\\n", but got "fire 3!\n', 'FAIL') 31 commits have nondeterministic ('systemtap.syscall/nd_syscall.exp', 'FAIL: 32-bit clock nd_syscall\n', 'FAIL') 31 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_enabled_iter_4 (invalid output)\n', 'FAIL') 31 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_enabled_iter_5 (invalid output)\n', 'FAIL') 31 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_disabled_iter_3 (invalid output)\n', 'FAIL') 30 commits have nondeterministic ('systemtap.syscall/syscall.exp', 'FAIL: 32-bit clock syscall\n', 'FAIL')&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article illustrates how Bunsen's Python library can be used to quickly develop analysis scripts to answer questions about a project's testing history. More generally, the example demonstrates the benefit of keeping a long-term archive of test results that can be used to answer questions about a project's testing history.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/09/detecting-nondeterministic-test-cases-bunsen" title="Detecting nondeterministic test cases with Bunsen"&gt;Detecting nondeterministic test cases with Bunsen&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serhei Makarov</dc:creator><dc:date>2022-06-09T07:00:00Z</dc:date></entry><entry><title type="html">Vlog: WildFly gRPC</title><link rel="alternate" href="https://www.youtube.com/watch?v=UYSNM9Dy5M4" /><author><name>Harald Pehl</name></author><id>https://www.youtube.com/watch?v=UYSNM9Dy5M4</id><updated>2022-06-09T00:00:00Z</updated><dc:creator>Harald Pehl</dc:creator></entry><entry><title>Eliminate downtime during OpenShift rolling updates</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/08/eliminate-downtime-during-openshift-rolling-updates" /><author><name>Rupesh Patel</name></author><id>97afecef-2ffb-499a-80dc-28374c1a1891</id><updated>2022-06-08T07:00:00Z</updated><published>2022-06-08T07:00:00Z</published><summary type="html">&lt;p&gt;Do your clients complain about interruptions during software upgrades? Do you observe connection failures or timeouts during those upgrades? In this article, you'll learn how you can minimize the impacts on your client visiting your services hosted on the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; during software updates.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;rolling update&lt;/em&gt; creates new pods running the new software and terminates old ones. The deployment controller performs this rollout incrementally, ensuring that a certain number of new pods are ready before the controller deletes the old pods that the new pods are replacing. For details, see &lt;a href="https://docs.openshift.com/container-platform/4.10/applications/deployments/deployment-strategies.html#deployments-rolling-strategy_deployment-strategies"&gt;Rolling strategy&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; documentation.&lt;/p&gt; &lt;p&gt;Figure 1 shows a typical sequence of events during an update. The important point, for the purposes of this article, is that pods go through a transitional period where they are present but not functional. Achieving a zero-downtime rollout requires some care to drain traffic from the old pods and allow the OpenShift router time to update its configuration before the deployment controller removes the old pods.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%20from%202022-04-26%2016-18-09.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screenshot%20from%202022-04-26%2016-18-09.png?itok=5x5Q9bwB" width="868" height="727" alt="A rolling update adds and removes pods, while the service points to both old and new pods." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A rolling update adds and removes pods, while the service points to both old and new pods. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: A rolling update adds and removes pods.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Pod termination starts with setting its &lt;code&gt;deletionTimestamp&lt;/code&gt; field to a non-null value to indicate that it has been marked for deletion. An &lt;code&gt;oc get&lt;/code&gt; or &lt;code&gt;kubectl get&lt;/code&gt; command shows such a pod in a &lt;code&gt;Terminating&lt;/code&gt; state. A pod may exist in this state for some period of time (several seconds or minutes, possibly even hours or days) before the pod is actually removed. See &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination"&gt;Termination of Pods&lt;/a&gt; in the Kubernetes documentation for details.&lt;/p&gt; &lt;p&gt;When a pod enters the &lt;code&gt;Terminating&lt;/code&gt; state, different parts of the system react to resolve the transitional status:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;The kubelet updates the pod's status to &lt;code&gt;Ready&lt;/code&gt;=&lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The endpoint slice controller observes the update to the pod's status and removes the pod's IP address from any &lt;code&gt;EndpointSlice&lt;/code&gt; object that has it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The OpenShift router observes this update to the &lt;code&gt;EndpointSlice&lt;/code&gt;. The router removes the pod's IP address from the &lt;a href="https://github.com/haproxy"&gt;HAProxy&lt;/a&gt; configuration to stop HAProxy from forwarding requests to the pod. Finally, the router reloads HAProxy so that the configuration changes take effect.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thus, there can be a delay between when the pod is marked for deletion and when the OpenShift router reloads HAProxy with the updated configuration.&lt;/p&gt; &lt;p&gt;How does this affect the risk of downtime? Suppose you have a route with some backend pod, and a client sends a request for that route. Any of the following can happen with that request:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;HAProxy forwards the request to the backend pod, and it remains responsive for the duration of the transaction. In this case, the pod sends a response, and HAProxy forwards the response to the client. Everything is fine.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;HAProxy forwards the request to the backend pod, and the pod is terminated during the transaction. In this case, HAProxy returns an error response to the client. This makes the service appear to be down, even though many other pods are running.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;HAProxy forwards the request to a backend pod that has already been terminated. In this case, the connection to the pod fails. Then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;If there is no other backend pod, HAProxy returns an error response to the client.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If there is another backend pod, HAProxy retries the request with that pod. In this case, the client gets a successful response, although it might be delayed while HAProxy's connection to the first backend pod fails and HAProxy retries the request with the other pod.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Solution: A PreStop container hook&lt;/h2&gt; &lt;p&gt;The risk of downtime can be almost completely eliminated through a simple solution: the introduction of an arbitrary delay during the &lt;code&gt;Terminating&lt;/code&gt; state so that a pod continues to accept and handle requests until HAProxy stops forwarding requests to that pod. This grace period can be added by adding a &lt;code&gt;PreStop&lt;/code&gt; hook to the deployment.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;PreStop&lt;/code&gt; hook simply delays pod termination in order to allow HAProxy to stop forwarding requests to it. In addition, if the application handles long-lived connections, the &lt;code&gt;PreStop&lt;/code&gt; hook must delay the pod's removal long enough for these connections to finish.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The application process itself may have a built-in termination grace period. In this case, adding a &lt;code&gt;PreStop&lt;/code&gt; hook would be superfluous.&lt;/p&gt; &lt;p&gt;If the application doesn't have long-lived connections, 15 to 30 seconds should be plenty of time for the &lt;code&gt;PreStop&lt;/code&gt; hook. The administrator should test the &lt;code&gt;PreStop&lt;/code&gt; hook with different values and set a value that suits their environment. It is crucial that the pod continues to respond to requests while it is in the &lt;code&gt;Terminating&lt;/code&gt; state.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The administrator should keep in mind that adding a &lt;code&gt;PreStop&lt;/code&gt; hook consumes more time for recycling pods than usual.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Graceful termination requires time. Rolling updates can take up to several minutes to complete. For certain applications, graceful termination doesn't provide value. Determining whether it is worthwhile, and how long the grace period needs to be to allow traffic to drain, is up to the administrator. When configured appropriately, graceful termination can improve the experience for your end users.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/08/eliminate-downtime-during-openshift-rolling-updates" title="Eliminate downtime during OpenShift rolling updates"&gt;Eliminate downtime during OpenShift rolling updates&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rupesh Patel</dc:creator><dc:date>2022-06-08T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.22.1 released!</title><link rel="alternate" href="https://blog.kie.org/2022/06/kogito-1-22-1-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/06/kogito-1-22-1-released.html</id><updated>2022-06-08T01:23:46Z</updated><content type="html">We are glad to announce that the Kogito 1.22.1 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * Added support for models including collections at Data Index service * Support to access HTTP headers in Serverless Workflows started using REST. * Debug logger for processes in quarkus dev mode BREAKING CHANGES * Codegen maven step should be disable or there is a likely chance openapi in Serverless Workflow will stop working (Quarkiverse integration is enabled with codegen maven step and is still in experimental phase, some specs are working, but others not) * To disable this experimental change, please remove the maven goals “generate-code” and “generate-code-tests” from the Quarkus Maven Plugin in your pom.xml file. This will keep the old OpenAPI integration feature in place.  If you want to use the experimental feature, please add the goals mentioned above to your pom.xml file. The properties now must be changed to "quarkus.rest-client.&lt;class fqdn&gt;.url". The class FQDN can be found in "target/generated-code/open-api-stream". In the package "api" you will find the generated REST stubs that you need to use to properly configure the extension. Please for more information and the . In the next version we will support version (also see the ), which will favor the Quarkus REST Client integration and will make it easy to properly configure this integration.  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.19.0 artifacts are available at the . A detailed changelog for 1.22.0 can be found in as well as for . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Kafka Distributions Landscape</title><link rel="alternate" href="http://www.ofbizian.com/2022/06/kafka-distributions-landscape.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/06/kafka-distributions-landscape.html</id><updated>2022-06-07T10:19:00Z</updated><content type="html">One aspect of Apache Kafka that makes it superior to other event streaming projects is not its technical features and performance characteristics, but the ecosystem that surrounds it. The number of books, courses, conference talks, Kafka service providers, consultancies, independent professionals, third-party tools and developer libraries that make up the Kafka landscape is unmatched by competing projects. While this makes Kafka a de facto standard for event streaming and provides assurance that it will be around for a long time to come, at the same time, Kafka alone is just a cog in the wheel and does not solve business problems on its own. This raises the question of which Kafka distributions are best suited to our use cases and which ecosystem will enable the highest productivity for our development teams and organizational constraints. In this post, we will try to navigate the growing ecosystem of Kafka distributions and give you some thoughts on where the industry is heading. KAFKA FOR LOCAL DEVELOPMENT If you are new to Kafka, you may assume that all you need is a Kafka cluster, and you are done. While this statement might be correct for organizations with a low level of Kafka adoption where Kafka is a generic messaging infrastructure, it does not represent the picture in the organizations with a higher level of event-streaming adoption where Kafka is used heavily by multiple teams in multiple sophisticated scenarios. The latter group needs developer productivity tools that offer rapid feedback during development of event-driven applications, high levels of automation, and repeatability in lower environments, and depending on the business priorities a variety of hybrid deployment mechanisms from edge to multiple clouds in production. The very first thing a developer team working heavily with stream processing applications would want is being able to start a short-lived Kafka quickly on their laptop. That is true regardless if you practice test-driven development and mock all external dependencies, or a rapid prototyping technique. As a developer, I want to quickly validate that my application is wiring up and functioning properly with an in-memory database or messaging system. Then I want to be able to do repeatable integration testing with a real Kafka broker. Having this rapid feedback cycle enables developers to iterate and deliver features faster and adapt to changing requirements. The good news is that there are a few projects addressing this need. The ones that I’m most familiar with are and from Spring in the Java ecosystem. The easiest way to unit test Kafka applications is with smallrye-messaging that replaces the channel implementation with . This has nothing to do with Kafka, but shows how using the right streaming abstraction libraries can help you unit test applications rapidly. Another option is to start an in-memory Kafka cluster in the same process as the test resource through to use that for a quick integration test. If you want to start a real Kafka broker as a separate process as part of the resource, Quarkus can do that through Dev Services for Kafka. With this mechanism, Quarkus will start a Kafka cluster in less than a second using containers. This mechanism can validate Kafka-specific aspects of your application and ensure it is working as expected on the local machine. The cool thing about Dev Services is that it can also start a schema registry (such as ), relational databases, caches, and many other 3rd party service dependencies. Once you are done with the “inner-loop” development step, you want to commit your application to a source control system and run some integration tests on the central build system. You can use to start a Kafka broker from a Java DSL (or for C), and allow you to pick specific Kafka distributions and versions. If your application passes all the gates, it is ready for deployment into a shared environment with other services where a continuously running Kafka cluster is needed. In this post, we are focusing only on the Kafka broker distributions and not the complete Kafka ecosystem of tools and additional components. There are other monitoring and management tools, and services that help developers and operations teams with their daily activities which we leave for another post. Self-managed Kafka Since our application has not reached production or a performance testing environment that requires production-like characteristics, all we want is to have a Kafka installation that is reliable enough for various teams to integrate and run some tests without involving a lot of effort to manage. Another characteristic of such an environment is to be low cost without the cost overhead of data replication and multi-AZ deployment. Many organizations have Kubernetes environments where each development team has their isolated namespace and shared namespaces for CI/CD purposes with all the shared dependencies deployed. project - origicnally created by Red Hat has everything needed to automate and operate a Kafka cluster on Kubernetes for development and production purposes. The advantage of using Strimzi for the lower environments is that it can be managed through a declarative Kubernetes API which is used by developers to manage the applications they develop and other 3’rd party dependencies. This allows developers to use the same Kubernetes infrastructure to quickly create a Kafka cluster for individual or team uses, a temporary project cluster, or a longer living shared cluster, repeatedly through automation pipelines and processes w/o going to depend on other teams for approval and provisioning of new services. List of Apache Kafka distributions and use cases Self-managed Kafka clusters are not used only for development purposes, but for production too. Once you get closer to a production environment, the characteristics required from the Kafka deployment change drastically. You want to be able to provision production-like Kafka clusters for application performance testing and DR testing scenarios. A production environment is not usually a single Kafka cluster either, it can be optimized for different purposes. You may want a self-managed Kafka cluster to deploy on your edge clusters that run offline, on-premise infrastructure that may require a non-standard topology or public cloud with a fairly standard multi-AZ deployment. And there are many self-managed Kafka platforms from Red Hat, Confluent, Cloudera, TIBCO, to name a few. The main characteristic of a self-managed cluster is the fact that the responsibility to manage and run the Kafka cluster resides within the organization owning the cluster. With that, a self-managed cluster also allows customization and configuration of the Kafka cluster, bespoke tuning to suit your deployment needs. For these and any other odd use cases that are not possible with model, the self-managed Kafka remains a proven path. KAFKA AS A SERVICE Each organization is different, but some of the common criteria for production Kafka clusters are things such as the ability to deploy on multiple AZs, on-demand scaling, compliance and certifications, a predictable cost model, open for 3rd party tools and services integrations, and so forth. Today, Kafka is over a decade old and there are multiple mature Kafka as a Service offerings able to satisfy many production needs. While these offerings vary in sizing options, the richness of the user interface, Kafka ecosystem components, and so forth, a key difference is whether Kafka is treated as an infrastructure component or treated as its own event-streaming category with its event-streaming abstractions. Apache Kafka distributions landscape Based on the abstraction criteria we can see that some SaaS providers (such as AWS MSK, Heroku, Instaclustr, Aiven) focus on infrastructure details such as VM sizes, the number of cores and memory, storage types, broker, Zookeeper, Kafka Connect details, and so forth. Many critical decisions about the infra choice, capacity matching to Kafka, Kafka cluster configurations, Zookeeper topology, are left for the user to decide. These services resemble infrastructure services that happen to run Kafka on top, and that is reflected in the VM-based sizing and pricing models. These services have a larger selection of sizing options and can be preferred by teams that have infrastructure inclination and preference to know and control all aspects of a service (even a managed service). Other Kafka as a Service providers (such as Confluent Cloud, AWS MSK Serverless, Red Hat Openshift Streams for Apache Kafka, Upstash) go the opposite “Kafka-first” direction where the infra, the management layer (typically Kubernetes based), and Kafka cluster details are taken care of, and hidden. With these services, the user is dealing with higher level, Kafka-focused abstractions such as Streaming/Kafka/Topic-like units of measure (which represents normalized multi-dimensional Kafka capacity) rather than infrastructure capacity; availability guarantees instead of deployment topology of brokers and Zookeeper; connectors to external systems as an API (regardless of the implementation technology) instead of Kafka Connect cluster deployment and connector deployments. This approach exposes what is important for a Kafka user and not the underlying infrastructure or implementation choices that make up a Kafka service. In addition, these Kafka-first services offer a consumption based Kafka-centric pricing model where the user pays for Kafka capacity used and quality of service rather than provisioned infrastructure with the additional Kafka margin. These services are more suitable for lines of business teams that focus on their business domain and treat Kafka as a commodity tool to solve the business challenges. Next, we will see why Kafka-first managed services are blurring the line and getting closer to a serverless-like Kafka experience where the user is interacting with Kafka APIs and everything else is taken care of. “SERVERLESS-LIKE” KAFKA Serverless technologies are a class of SaaS that have specific characteristics offering additional benefits to users such as a pay-per-use pricing model and eliminating the need for capacity management and scaling altogether. This is achieved through things such as not having to provision and manage servers, built-in high availability, built-in rebalancing, automatic scaling up, and scaling down to zero. We can look at the “Serverless Kafka” category from two opposing angles. On the positive side, we can say that the “Kafka-first” services are getting pretty close to a serverless user experience except for the pricing aspect. With these Kafka-first services, users don't have to provision infrastructure, the Kafka clusters are already preconfigured for high availability, with partition rebalancing, storage expansion, and auto-scaling (within certain boundaries). On the negative side, whether a Kafka service is called serverless or not, these offerings still have significant technical and pricing limitations and they are not mature enough. These services are constrained in terms of message size, partition count, partition limit, network limit, storage limit. These constraints limit the use cases where a so-called serverless Kafka can be used. Other than Upstash which is charging per message, the remaining serverless Kafka services charge for cluster hours which is against the scale-to-zero/pay-per-use ethos of the serverless definition. That is why today I consider the serverless Kafka category still an inspiration rather than reality. Nevertheless, these trends set the direction where managed Kafka offerings are headed: that is complete infrastructure and deployment abstractions hidden from the user; Kafka-first primitives for capacity, usage, quality of a service; autonomous service lifecycle that doesn’t require any user intervention; and with a true pay-for-what-you-use pricing model. SUMMARY How many types of Kafka do you need? The answer is more than one. You want developer frameworks that can emulate and enable rapid, iterative development. You want a declarative and automated way to repeatedly deploy and update development environments. Depending on your business requirements, you may require highly customised Kafka implementations at the edge or standard implementations across multiple clouds that are all connected. While your organization's event streaming adoption and Kafka maturity grows, you will need more Kafka. But there is a paradox. If you are not in the Kafka business, you should work less on Kafka itself and use Kafka for more tasks that set your business apart. This is possible if you use Kafka through higher-level frameworks like Strimzi that automate many of the operational aspects, or through a that takes care of low-level decision-making and relieves you of the responsibility of running Kafka. This way, your teams stop thinking about Kafka and start thinking about how to use Kafka for what matters to your customers. Follow me at to join my journey of learning Apache Kafka. This post was originally published on Red Hat Developers. To read the original post, check and .</content><dc:creator>Unknown</dc:creator></entry><entry><title>Thousands of PyPI and RubyGems RPMs now available for RHEL 9</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9" /><author><name>Miroslav Suchý</name></author><id>28c0e94c-2e99-4c92-ba50-262890b329f8</id><updated>2022-06-07T07:00:00Z</updated><published>2022-06-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9 now offers convenient (but unsupported) access to RPMs from two of the largest and most popular code repositories: The &lt;a href="https://pypi.org"&gt;Python Package Index&lt;/a&gt; (PyPI) for &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; and the &lt;a href="https://rubygems.org"&gt;RubyGems&lt;/a&gt; collection for &lt;a href="https://developers.redhat.com/topics/ruby/all"&gt;Ruby&lt;/a&gt;. This new offering makes it easier to use thousands of community libraries in your projects. We'll look at the repositories in this article.&lt;/p&gt; &lt;h2&gt;The Red Hat repository ecosystem and COPR&lt;/h2&gt; &lt;p&gt;Red Hat supports about 2,500 packages. They are of very high quality, created and maintained by experts. Outside our supported offerings for RHEL 9, we also have &lt;a href="https://www.redhat.com/en/blog/whats-epel-and-how-do-i-use-it"&gt;Extra Packages for Enterprise Linux&lt;/a&gt; (EPEL 9) with an additional 3,000 packages. But there are a lot of other important libraries and utilities on the internet.&lt;/p&gt; &lt;p&gt;We certainly cannot package everything. But we have decided to provide you with an additional 151,000 RPM packages by packaging everything we can (with an exception we'll discuss in a moment) in PyPI and RubyGems. The packages have been added to another set of unsupported packages called &lt;a href="https://copr.fedorainfracloud.org/"&gt;COPR&lt;/a&gt;, which stands for &lt;strong&gt;co&lt;/strong&gt;mmunity &lt;strong&gt;pr&lt;/strong&gt;ojects. Both EPEL and COPR were formed by the &lt;a href="https://fedoraproject.org/"&gt;Fedora project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Benefits of using the signed Red Hat repositories&lt;/h2&gt; &lt;p&gt;As noted, Red Hat does not offer support for COPR packages. Each is provided as-is, without warranty. If you have an issue with some package, please contact an upstream author.&lt;/p&gt; &lt;p&gt;However, Red Hat does provide some security through signatures. Packages on PyPI &lt;a href="https://github.com/pypa/pip/issues/425"&gt;are not signed&lt;/a&gt;, but Red Hat signs the RPM packages in COPR repositories. This means that you can audit the packages: You know which files belong to which package and vice versa, and you will be able to check whether a file was altered by a user.&lt;/p&gt; &lt;p&gt;To enable the PyPI COPR on your computer, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# dnf copr enable @copr/PyPI epel-9-x86_64&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, if &lt;code&gt;pip install foo&lt;/code&gt; works for you, installing &lt;code&gt;python3-foo&lt;/code&gt; from this repository will work too.&lt;/p&gt; &lt;p&gt;&lt;span&gt; &lt;/span&gt;To enable the RubyGems COPR, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# dnf copr enable @rubygems/rubygems epel-9-x86_64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having enabled the COPRs on your system, you can get the signed keys by entering:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# dnf install distribution-gpg-keys # rpm --import /usr/share/distribution-gpg-keys/copr/copr-@copr-PyPI.gpg # rpm --import /usr/share/distribution-gpg-keys/copr/copr-@rubygems-rubygems.gpg&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Details and limitations&lt;/h2&gt; &lt;p&gt;Not all packages from PyPI and RubyGems could be included in our COPRs. Some packages suffered from build problems, which I'll describe later. Others were excluded due to licensing conflicts. If the maintainers of a package fail to explicitly assign a free or &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; license, they leave the gem by default under conventional "All rights reserved" copyright. Because these packages are not open source, we cannot distribute them.&lt;/p&gt; &lt;p&gt;We provide the latest version of each Python package and RubyGem. When a new version is published, we rebuild it and update the repository. The previous version is deleted after 14 days. If you need an older version, upload it to &lt;a href="https://access.redhat.com/products/red-hat-satellite"&gt;Red Hat Satellite&lt;/a&gt; or keep a local copy.&lt;/p&gt; &lt;h2&gt;What we built from PyPI&lt;/h2&gt; &lt;p&gt;Many improvements to Python packaging were made in recent years, both upstream and downstream in RPM.&lt;/p&gt; &lt;p&gt;With new RPM macros created by Python maintainers in Red Hat, it is now possible to create a deterministic converter from Python package metadata to RPM specfiles. We used a new tool, &lt;a href="https://github.com/befeleme/pyp2spec/"&gt;pyp2spec&lt;/a&gt;, making use of those new RPM macros, to rebuild PyPI packages as RPMs in COPR.&lt;/p&gt; &lt;p&gt;Starting in December 2021, we tried to rebuild all packages on PyPI (more than 330,000 at the time) in Fedora Rawhide. When it became possible to build packages for EPEL 9 in COPR, we went ahead and reran the build. We completed this project, building 79,842 packages for Red Hat Enterprise Linux 9 and making them available in the &lt;a href="https://copr.fedorainfracloud.org/coprs/g/copr/PyPI/"&gt;PyPI COPR&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Only the packages that were built successfully for Fedora Rawhide were submitted for the Red Hat Enterprise Linux 9 rebuild. Trying to rebuild the whole PyPI repository would require months, and there was little chance that any given EPEL build would succeed if the Rawhide one didn't.&lt;/p&gt; &lt;p&gt;Based on the build logs of the failed packages, we found the following problems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;32% had missing build dependencies in our COPR repository, often because a package required a specific version of a dependency.&lt;/li&gt; &lt;li&gt;20% didn't have a license or specified a license that wasn't detected as open source.&lt;/li&gt; &lt;li&gt;About 13% of packages had an error somewhere in the upstream configuration leading to the build failure. Typically, the problem was due either to missing files in the source archive or to failures to import modules that weren't declared as build dependencies.&lt;/li&gt; &lt;li&gt;12% of packages didn't have the source archive uploaded to PyPI, which prevented us from building the RPM. If a package was built successfully for Rawhide but not for EPEL, the typical culprit was missing dependencies.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you encounter a package that's installable from &lt;code&gt;pip&lt;/code&gt;, but is not available in our COPR, you can try contacting the upstream maintainers to discuss loosening version requirements for dependencies, fixing the license declaration, uploading the source archive to PyPI, or resolving other problems as the case may be.&lt;/p&gt; &lt;p&gt;For instructions on how to reproduce the build, see our &lt;a href="https://copr.fedorainfracloud.org/coprs/g/copr/PyPI/"&gt;PyPI COPR&lt;/a&gt; repository.&lt;/p&gt; &lt;h2&gt;What we built from RubyGems&lt;/h2&gt; &lt;p&gt;Support for building RPM packages in COPR directly from RubyGems.org was &lt;a href="http://frostyx.cz/posts/copr-rubygems"&gt;introduced back in 2016&lt;/a&gt;. The procedure uses a tool called &lt;a href="https://github.com/fedora-ruby/gem2rpm"&gt;gem2rpm&lt;/a&gt; to convert gem metadata into an RPM specfile and produce an SRPM package based on it.&lt;/p&gt; &lt;p&gt;Utilizing this feature, we have rebuilt all of RubyGems.org for Fedora Rawhide. In &lt;a href="http://frostyx.cz/posts/rebuilding-the-entire-rubygems-in-copr"&gt;this detailed blog post&lt;/a&gt;, you can find more information about the success rate, the size of the packages and their metadata, COPR internals, and takeaways for the RPM toolchain.&lt;/p&gt; &lt;p&gt;Countless performance improvements, and months of building later, we are now announcing that all of RubyGems.org was rebuilt for Red Hat Enterprise Linux 9. The &lt;a href="https://copr.fedorainfracloud.org/coprs/g/rubygems/rubygems"&gt;RubyGems project in COPR&lt;/a&gt; provides 71,952 packages, which is almost half of the RubyGems.org service. We got 19,635 failures because of unmet dependencies, and around 77,000 gems were skipped and not even attempted to be built because of missing licenses. A full 37% of gems in RubyGems do not specify a &lt;a href="https://guides.rubygems.org/specification-reference/#license="&gt;license&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If your gem isn't available from the RubyGems COPR, it is most likely due to a missing license. Please resolve such issues with the respective gem owners. The same applies to missing dependencies. If you find a problem with the generated specfiles, please file &lt;a href="https://github.com/fedora-ruby/gem2rpm/issues"&gt;a new issue&lt;/a&gt; for &lt;code&gt;gem2rpm&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our PyPI and RubyGems projects demonstrate Red Hat's desire to help programmers make the most of free and open source resources. During our months-long efforts, we turned up weaknesses in the source repositories that guide upstream developers to produce more robust packages.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9" title="Thousands of PyPI and RubyGems RPMs now available for RHEL 9"&gt;Thousands of PyPI and RubyGems RPMs now available for RHEL 9&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Miroslav Suchý</dc:creator><dc:date>2022-06-07T07:00:00Z</dc:date></entry><entry><title type="html">Creating tabs using DashBuilder</title><link rel="alternate" href="https://blog.kie.org/2022/06/creating-tabs-using-dashbuilder%ef%bf%bc.html" /><author><name>Nikhil Dewoolkar</name></author><id>https://blog.kie.org/2022/06/creating-tabs-using-dashbuilder%ef%bf%bc.html</id><updated>2022-06-06T16:47:50Z</updated><content type="html">DashBuilder is a full-featured web application that allows non-technical users and programmers to create business dashboards. Dashboard data can be extracted from heterogeneous sources of information such as Prometheus, JDBC databases, or regular text files. The resulting dashboard can be deployed in a cloud environment using DashBuilder Runtime. In this post, we will walk you through creating tabs using DashBuilder to facilitate  better organization of your dashboards. Home Page of dashbuilder Adding dataset Step 1:- To create tablists, we should add a dataset to DashBuilder. Click on the datasets option and then click on the new dataset. You will get options for dataset types like Bean, CSV, Json, SQL,Prometheus, Kafka, External and Execution server. For this blog post, we will use CSV as an option. Dataset List Page Step 2: Select dataset type(we are using CSV as dataset for this example).Going forward to the next step, you will land on the following screen. Enter name for dataset, then select CSV file and then click on upload button which is beside the file select button. Enter the correct separation char(in this example comma (,) is separation char) and quote char(in this example comma (“) is quote char) and then click on the test button. Dataset link:- Adding CSV Dataset Step 3:- You will now get the following screen. If you want to remove some columns then unselect the columns else click on the Next button.  Enter a comment and click on the Save button. Finalizing columns in datasets Creating New Pages and group for tablists:- Step 1:- Creating new pages procedure is available in the following post:- Create 4-5 pages with some design in it so that we can create a tablist using the navigation page. After creating pages you will see following screen Create Pages to be added to tablist Step 2:- Go to the navigation option icon which is just below the pages icon.     Navigation Page Step3:- Click on the setting icon in front of dashboards and click on the new group. And give a group name to it.  Now, click on the setting icon again in front of your created group. Add pages which you have created earlier using the pages icon and select pages from the dropdown list.You can now add all the pages to the group. Creating new group Creating Tablist:- Step 1:- Go to pages option and create one page to add tablist to it. Then drag and drop target div to pages. Give it a name. Creating new page to add tablist Adding target div to page Step 2:- Under the navigation option drag and drop the Tablist to page. Select the proper group option then give the default page name and select the target div you created. Click on Ok . Your tablist will be created. The following pictures will explain this step fully. Adding tablist to Page  Creating new tablist     Tablist added Conclusion:- This blogpost explains all the steps to create a tablist in DashBuilder.  Don’t forget to check out the newest addition to DashBuilder.i.e, the YML editor which doesn’t require any extra installation on your end. Creating a dashboard using pure YML from data from any JSON document is now possible using Dashbuilder! You are just required to head over to the and start your dashboard! You can check out the sample dashboards built using YML editor on the . If you want to create your own dashboards, feel free to refer to the . Feel free to explore all and let us know in the comments section if this post was of some help. The post appeared first on .</content><dc:creator>Nikhil Dewoolkar</dc:creator></entry><entry><title>What's new in version 2.7 of the Red Hat build of Quarkus</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/06/whats-new-version-27-red-hat-build-quarkus" /><author><name>Jeff Beck</name></author><id>2d202f19-5acc-4180-9b53-143943e3e72e</id><updated>2022-06-06T07:00:00Z</updated><published>2022-06-06T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat recently released version 2.6 of the Red Hat build of Quarkus to support enterprise developers building &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;-native &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; applications. The latest release has several great new features and performance improvements, including tools to improve developer productivity.&lt;/p&gt; &lt;p&gt;Let’s take a look at some highlights from this release. For a complete list, check out the &lt;a href="http://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/quarkus-2-7/guide/5cbab82b-042a-4a68-ac5e-54901a9cc222"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Developer productivity&lt;/h2&gt; &lt;p&gt;One of the founding principles of Quarkus is to bring joy to Java developers by creating a frictionless experience through a combination of tools, libraries, extensions, and more. This release continues that mission by making developers more efficient with new tools and services such as Hibernate search, dev services, and a Kafka UI.&lt;/p&gt; &lt;h3&gt;Hibernate search&lt;/h3&gt; &lt;p&gt;Hibernate search is now fully supported, providing a powerful tool for free text search capability to data. Normal Jakarta Persistence API (JPA) queries are limited to SQL queries which only query per column. With Hibernate search, developers can easily express how their Java model should be indexed and searchable. Hibernate search can also extend web page search to a full-text search of your Java domain, including searches for synonyms, sound-alike words, and more. Check out the &lt;a href="https://quarkus.io/guides/hibernate-search-orm-elasticsearch"&gt;documentation&lt;/a&gt; and the &lt;a href="https://www.youtube.com/watch?v=hwxWx-ORVwM"&gt;Quarkus Insights session&lt;/a&gt; to learn more, or watch this video on Hibernate Search from Red Hat Developer.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h3&gt;Dev services&lt;/h3&gt; &lt;p&gt;When an application starts in dev mode, dev services provide automatic provisioning of unconfigured services, such as a database or identity server. When Quarkus adds the service extension, it automatically starts the relevant service and wires your application to use it. This release adds to the growing list of dev services, including &lt;a href="https://quarkus.io/guides/dev-services#neo4j"&gt;Neo4j&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/dev-services#keycloak"&gt;Keycloak&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/dev-services#infinispan"&gt;Infinispan&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/dev-services#vault"&gt;Vault&lt;/a&gt;, and &lt;a href="https://quarkus.io/guides/dev-services"&gt;more&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Kafka UI&lt;/h3&gt; &lt;p&gt;The Kafka UI (Figures 1 and 2) allows developers to visualize Apache Kafka Streams for reactive applications within the &lt;a href="https://quarkus.io/guides/dev-ui"&gt;Dev UI&lt;/a&gt;. The UI shows how the event streams sink in topics and how applications consume the streams from topics. The UI illustrates how the application aggregates streams from multiple topics to a single topic. The UI also showcases the continuous sequence of sourcing, joining, and aggregating streams in Kafka clusters. The Red Hat Developer article &lt;a href="https://developers.redhat.com/articles/2021/12/07/visualize-your-apache-kafka-streams-using-quarkus-dev-ui"&gt;Visualize your Apache Kafka Streams using the Quarkus Dev UI&lt;/a&gt; has more on this topic.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dev_1.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dev_1.jpg?itok=99ED6Ycu" width="600" height="326" alt="Quarkus Dev UI with Kafka Streams" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Dev UI Console &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Dev UI console.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic_0_0.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topic_0_0.jpg?itok=M42ix0hv" width="600" height="403" alt="Quarkus Kafka UI" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Quarkus Kafka UI &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Quarkus Kafka UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Performance and framework efficiency&lt;/h2&gt; &lt;p&gt;Quarkus makes Java supersonic and subatomic, with fast startup times, low memory footprint, and a small disk footprint. This release continues to improve the Quarkus framework's performance and efficiency.&lt;/p&gt; &lt;h3&gt;UPX&lt;/h3&gt; &lt;p&gt;UPX is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt;, portable, high-performance executable packer that takes an executable as input and produces a compressed executable. Quarkus can now automatically perform UPX compression as part of the &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; creation process. By using UPX to compress a native executable, developers can build containers that are smaller than 30MB with Quarkus in them. &lt;a href="https://quarkus.io/blog/upx/"&gt;Learn more&lt;/a&gt; about compressing native executables using UPX.&lt;/p&gt; &lt;h2&gt;Kubernetes-Native&lt;/h2&gt; &lt;p&gt;Quarkus enables Java developers to create performant, easily deployable and maintainable applications on Kubernetes and the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. This release includes many impactful Kubernetes-Native features.&lt;/p&gt; &lt;h3&gt;SmallRye Stork&lt;/h3&gt; &lt;p&gt;In a distributed &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice architecture&lt;/a&gt;, discovery and load balancing can be challenging with interactions between services. Developers can use Stork (currently in tech preview) to fully abstract away the service discovery for Consul, Eureka, Kubernetes/OpenShift, or static machines. Learn more about how to use SmallRye Stork in the &lt;a href="https://quarkus.io/guides/stork"&gt;Quarkus guide&lt;/a&gt;, or check out this Red Hat Developer video on the subject.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Community and standards&lt;/h2&gt; &lt;p&gt;Quarkus starts and ends with the community. Quarkus includes a vast ecosystem of over 400 extensions and support for popular Java APIs and standards. This release continues that mission with new features and standards support.&lt;/p&gt; &lt;h3&gt;Java 17 support&lt;/h3&gt; &lt;p&gt;This release includes improved developer tools for using Java 17. Now you can &lt;a href="https://code.quarkus.redhat.com/"&gt;generate projects&lt;/a&gt; based on Java 17 without manually configuring the version. This release also includes a Technology Preview for Java 17 in native mode (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Untitled%20presentation%20%287%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Untitled%20presentation%20%287%29.png?itok=LAd5kUss" width="600" height="338" alt="Quarkus code generator for Java 17" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Quarkus code generator for Java 17 &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Quarkus code generator for Java 17.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Try Quarkus&lt;/h2&gt; &lt;p&gt;The best way to experience the new features of Quarkus is to start using it. Generate your code at &lt;a href="http://code.quarkus.redhat.com"&gt;code.quarkus.redhat.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can find more Quarkus resources on Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/e-books/quarkus-spring-developers"&gt;Quarkus for Spring Developers&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/books/quarkus-cookbook"&gt;Quarkus Cookbook&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Learn Quarkus faster with quick starts in the &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/06/whats-new-version-27-red-hat-build-quarkus" title="What's new in version 2.7 of the Red Hat build of Quarkus"&gt;What's new in version 2.7 of the Red Hat build of Quarkus&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jeff Beck</dc:creator><dc:date>2022-06-06T07:00:00Z</dc:date></entry><entry><title type="html">Creating Quarkus applications using IntelliJ IDEA</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/creating-quarkus-projects-using-intellij-idea/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/creating-quarkus-projects-using-intellij-idea/</id><updated>2022-06-06T05:22:00Z</updated><content type="html">This tutorial introduces you to JetBrains IntelliJ Quarkus plugin which lets you bootstrap and develop Quarkus projects in the simplest and most productive way. Today, there is a wealth of plugins available to develop applications with Quarkus (including Visual Studio, Eclipse, Eclipse Che and IntelliJ Idea). This article focuses on IntelliJ Plugin for Quarkus which ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
